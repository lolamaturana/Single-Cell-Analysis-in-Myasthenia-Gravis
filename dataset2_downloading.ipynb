{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi3V2UnoazVB",
        "outputId": "2f470582-905c-4972-a036-e1a95303757d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.3/183.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m641.1/641.1 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m161.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.5/244.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m141.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docrep (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pims (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "distributed 2025.9.1 requires dask==2025.9.1, but you have dask 2024.11.2 which is incompatible.\n",
            "rapids-dask-dependency 25.10.0 requires dask==2025.9.1, but you have dask 2024.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet scanpy pandas numpy anndata igraph leidenalg squidpy scvi-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jlzGPhvbbs8o"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import leidenalg\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import anndata as ad\n",
        "import scipy.sparse as sp\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UniB9qcsbtiu",
        "outputId": "957aaff0-9ef7-429e-aed4-33eb82fa1c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nIBealaWld_3"
      },
      "outputs": [],
      "source": [
        "sc.settings.verbosity = 3 #for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-lwOYjculhZf"
      },
      "outputs": [],
      "source": [
        "sc.settings.set_figure_params(figsize=(5,5)) #standardize figure sizes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checks = True #variable that determines whether we print checks used during coding or not"
      ],
      "metadata": {
        "id": "6Y_l-hg01gZP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1osLl3bLcQ1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35e9a4e-50f3-4935-c271-a80f987743cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original metadata rows: 64650\n",
            "Cells in expression file: 64649\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "#paths to raw files downoaded from the dataset\n",
        "raw_path  = \"/content/drive/MyDrive/data_science_final_project/raw_data/expression_raw.csv.gz\"\n",
        "meta_path = \"/content/drive/MyDrive/data_science_final_project/raw_data/meta_dataset2.csv\"\n",
        "\n",
        "#load metadata\n",
        "meta = pd.read_csv(meta_path, index_col=0)\n",
        "print(\"Original metadata rows:\", meta.shape[0])\n",
        "\n",
        "#load header from CSV (cell barcodes)\n",
        "with gzip.open(raw_path, \"rt\") as f:\n",
        "    header = f.readline().strip().split(\",\")\n",
        "\n",
        "cells_data = header[1:]  #skip first empty field\n",
        "print(\"Cells in expression file:\", len(cells_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean Metadata:\n",
        "#keeep only rows whose index exists in the expression data\n",
        "valid_metadata = meta.index.intersection(cells_data)\n",
        "\n",
        "#drop invalid metadata rows such as \"TYPE\"\n",
        "if len(valid_metadata) < meta.shape[0]:\n",
        "    dropped = set(meta.index) - set(valid_metadata)\n",
        "    print(\"Dropping metadata rows not found in expression file:\", list(dropped))\n",
        "    meta = meta.loc[valid_metadata]\n",
        "\n",
        "#update list of metadata cells\n",
        "cells_meta = meta.index.tolist()\n",
        "print(\"Cleaned metadata rows:\", len(cells_meta))\n",
        "\n",
        "\n",
        "if(checks == True):\n",
        "  #check that all metadata cells exist in expression matrix\n",
        "  missing = set(cells_meta) - set(cells_data)\n",
        "  if missing:\n",
        "      print(\"ERROR: These metadata cells are still missing in expression matrix:\", list(missing)[:10])\n",
        "  else:\n",
        "      print(\"Metadata cells all found in expression matrix.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7CXgVeT350_",
        "outputId": "56f72fc9-4f5c-489d-ad5b-ac81a3a06d78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping metadata rows not found in expression file: ['TYPE']\n",
            "Cleaned metadata rows: 64649\n",
            "Metadata cells all found in expression matrix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W7JYnK1ZrT_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4dab98-41cc-4f46-8a50-e1e5b61828b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of cells_meta: <class 'pandas.core.frame.DataFrame'>\n",
            "Type of cells_data: <class 'list'>\n",
            "Example element from cells_meta: biosample_id\n",
            "Example element from cells_data: AAACCCACAACAGCTT\n",
            "Metadata cells all found in expression matrix\n",
            "Original metadata rows: 64649\n",
            "Cells in expression file: 64649\n"
          ]
        }
      ],
      "source": [
        "if(checks == True):\n",
        "  #print types before final consistency check\n",
        "  print(\"Type of cells_meta:\", type(meta))\n",
        "  print(\"Type of cells_data:\", type(cells_data))\n",
        "\n",
        "  #if they're lists/arrays/Series, print element type examples:\n",
        "  try:\n",
        "      print(\"Example element from cells_meta:\", next(iter(meta)))\n",
        "  except Exception:\n",
        "      print(\"Could not preview cells_meta\")\n",
        "\n",
        "  try:\n",
        "      print(\"Example element from cells_data:\", next(iter(cells_data)))\n",
        "  except Exception:\n",
        "      print(\"Could not preview cells_data\")\n",
        "\n",
        "  #original check\n",
        "  missing = set(meta.index) - set(cells_data)\n",
        "  if missing:\n",
        "      print(\" ERROR: These metadata cells are still missing in expression matrix:\", list(missing)[:10])\n",
        "  else:\n",
        "      print(\"Metadata cells all found in expression matrix\")\n",
        "\n",
        "  print(\"Original metadata rows:\", len(meta))\n",
        "  print(\"Cells in expression file:\", len(cells_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pbuvG_oCcXGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fda662-71b1-469a-8169-a4a399bb57de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total genes in matrix: 36601\n",
            "Will generate 10 chunks, each of ~3661 genes.\n"
          ]
        }
      ],
      "source": [
        "#count lines in file to determine number of genes\n",
        "TOTAL_GENES = sum(1 for _ in gzip.open(raw_path, \"rt\")) - 1  # minus the header\n",
        "print(\"Total genes in matrix:\", TOTAL_GENES)\n",
        "\n",
        "N_CHUNKS = 10\n",
        "chunk_size = TOTAL_GENES // N_CHUNKS + 1\n",
        "\n",
        "print(f\"Will generate {N_CHUNKS} chunks, each of ~{chunk_size} genes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5LF_ddWcXwE",
        "outputId": "2a05b21d-b7a7-4203-957b-6750e83a35da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chunked streaming (10 chunks)...\n",
            "Saving chunk 0 with 3661 genes...\n",
            "   Writing to /content/drive/My Drive/data_science_final_project/data_processed/chunks/chunk_0.h5ad ...\n",
            "Saved chunk 0 to Google Drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "import scipy.sparse as sp\n",
        "\n",
        "chunk_idx = 0\n",
        "chunk_files = []\n",
        "\n",
        "#chunk folder\n",
        "drive_chunk_dir = \"/content/drive/My Drive/data_science_final_project/data_processed/chunks\"\n",
        "os.makedirs(drive_chunk_dir, exist_ok=True)\n",
        "\n",
        "#map cell position in CSV to index\n",
        "cell_index_data = {cell: i for i, cell in enumerate(cells_data)}\n",
        "\n",
        "with gzip.open(raw_path, \"rt\") as f:\n",
        "    next(f)  #skip header\n",
        "\n",
        "    gene_names = []\n",
        "    data = []\n",
        "    rows = []\n",
        "    cols = []\n",
        "    gene_counter = 0\n",
        "\n",
        "    print(\"Starting chunked streaming (10 chunks)...\")\n",
        "\n",
        "    for line in f:\n",
        "        parts = line.strip().split(\",\")\n",
        "        gene = parts[0]\n",
        "        expr = parts[1:]\n",
        "\n",
        "        gene_idx = len(gene_names)\n",
        "        gene_names.append(gene)\n",
        "\n",
        "        #collect sparse non-zero entries\n",
        "        for j, val in enumerate(expr):\n",
        "            if val not in (\"0\", \"0.0\", \"\"):\n",
        "                rows.append(j)\n",
        "                cols.append(gene_idx)\n",
        "                data.append(float(val))\n",
        "\n",
        "        gene_counter += 1\n",
        "\n",
        "        #save chunk when full\n",
        "        if gene_counter % chunk_size == 0:\n",
        "            print(f\"Saving chunk {chunk_idx} with {len(gene_names)} genes...\")\n",
        "\n",
        "            #build sparse matrix (cells × genes_chunk)\n",
        "            X = sp.csr_matrix(\n",
        "                (data, (rows, cols)),\n",
        "                shape=(len(cells_data), len(gene_names))\n",
        "            )\n",
        "\n",
        "            #AnnData\n",
        "            ad = sc.AnnData(\n",
        "                X,\n",
        "                obs=pd.DataFrame(index=cells_meta),\n",
        "                var=pd.DataFrame(index=gene_names)\n",
        "            )\n",
        "\n",
        "            #save directly to Google Drive\n",
        "            fname = os.path.join(drive_chunk_dir, f\"chunk_{chunk_idx}.h5ad\")\n",
        "            print(f\"   Writing to {fname} ...\")\n",
        "            ad.write(fname)\n",
        "            print(f\"Saved chunk {chunk_idx} to Google Drive\")\n",
        "\n",
        "            chunk_files.append(fname)\n",
        "\n",
        "            #reset buffers\n",
        "            gene_names = []\n",
        "            data = []\n",
        "            rows = []\n",
        "            cols = []\n",
        "            chunk_idx += 1\n",
        "\n",
        "    #save final partial chunk\n",
        "    if len(gene_names) > 0:\n",
        "        print(f\"Saving final chunk {chunk_idx} with {len(gene_names)} genes...\")\n",
        "\n",
        "        X = sp.csr_matrix(\n",
        "            (data, (rows, cols)),\n",
        "            shape=(len(cells_data), len(gene_names))\n",
        "        )\n",
        "\n",
        "        ad = sc.AnnData(\n",
        "            X,\n",
        "            obs=pd.DataFrame(index=cells_meta),\n",
        "            var=pd.DataFrame(index=gene_names)\n",
        "        )\n",
        "\n",
        "        fname = os.path.join(drive_chunk_dir, f\"chunk_{chunk_idx}.h5ad\")\n",
        "        print(f\"Writing to {fname} ...\")\n",
        "        ad.write(fname)\n",
        "        print(f\"Saved final chunk {chunk_idx} to Google Drive\")\n",
        "\n",
        "        chunk_files.append(fname)\n",
        "\n",
        "print(\"Finished creating 10 gene-chunks.\")\n",
        "print(\"Chunks saved to:\", drive_chunk_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYvCYFk1ca-p"
      },
      "outputs": [],
      "source": [
        "#Concatinate chunks together\n",
        "\n",
        "import scanpy as sc\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "#Path where chunks were saved\n",
        "drive_chunk_dir = \"/content/drive/My Drive/DataScienceFinalProject/data/chunks/\"\n",
        "\n",
        "#List and sort chunk files\n",
        "chunk_files = sorted([\n",
        "    os.path.join(drive_chunk_dir, f)\n",
        "    for f in os.listdir(drive_chunk_dir)\n",
        "    if f.startswith(\"chunk_\") and f.endswith(\".h5ad\")\n",
        "])\n",
        "\n",
        "print(f\"Found {len(chunk_files)} chunk files.\")\n",
        "print(\"Example files:\", chunk_files[:3])\n",
        "\n",
        "\n",
        "#Load chunks\n",
        "print(\"Loading chunks from Google Drive...\")\n",
        "adatas = []\n",
        "for idx, f in enumerate(chunk_files):\n",
        "    print(f\"   Loading chunk {idx} ---{os.path.basename(f)}\")\n",
        "    ad = sc.read_h5ad(f)\n",
        "    adatas.append(ad)\n",
        "print(\"All chunks loaded.\")\n",
        "\n",
        "\n",
        "#Concatenate horizontally (genes axis)\n",
        "print(\"\\nConcatenating chunks horizontally (axis=1)...\")\n",
        "adata_full = sc.concat(adatas, axis=1, join=\"outer\")\n",
        "print(\"Shape after concat:\", adata_full.shape)\n",
        "\n",
        "\n",
        "#Reorder rows to match metadata order\n",
        "print(\"Reordering rows to match metadata...\")\n",
        "adata_full = adata_full[cells_meta, :]\n",
        "print(\"   Shape after reordering:\", adata_full.shape)\n",
        "\n",
        "\n",
        "#attach metadata\n",
        "print(\"Attaching metadata...\")\n",
        "adata_full.obs = meta.copy()\n",
        "print(\"Metadata attached.\")\n",
        "\n",
        "\n",
        "#Add counts layer\n",
        "print(\"Adding counts layer...\")\n",
        "adata_full.layers[\"counts\"] = adata_full.X.copy()\n",
        "print(\"Counts layer added.\")\n",
        "\n",
        "\n",
        "#Save final dataset to Drive\n",
        "output_path = \"/content/drive/My Drive/DataScienceFinalProject/data/dataset2_full.h5ad\"\n",
        "print(f\"\\nSaving final AnnData to: {output_path}\")\n",
        "adata_full.write(output_path)\n",
        "\n",
        "print(\"Final shape (cells × genes):\", adata_full.shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}